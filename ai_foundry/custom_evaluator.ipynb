{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Evaluators with Azure AI Foundry\n",
    "\n",
    "This notebook demonstrates how to evaluate data using custom evaluators and send the results to [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-studio/what-is-ai-studio).\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- An Azure subscription.\n",
    "- An Azure AI Foundry workspace.\n",
    "- An Azure AI Foundry project.\n",
    "- An Azure OpenAI resource.\n",
    "\n",
    "### Install the required packages\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Create the following environment variables or add them to an `.env` file\n",
    "\n",
    "```bash\n",
    "AZURE_OPENAI_ENDPOINT=<your-azure-openai-endpoint>\n",
    "AZURE_OPENAI_API_KEY=<your-azure-openai-api-key>\n",
    "AZURE_OPENAI_DEPLOYMENT=<your-azure-openai-deployment>\n",
    "AZURE_OPENAI_API_VERSION=<your-azure-openai-api-version>\n",
    "AZURE_SUBSCRIPTION_ID=<your-azure-subscription-id>\n",
    "AZURE_RESOURCE_GROUP=<your-azure-resource-group>\n",
    "AZURE_AI_FOUNDRY_PROJECT=<your-azure-azure_foundry_project>\n",
    "```\n",
    "\n",
    "### References\n",
    "\n",
    "- [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-studio/what-is-ai-studio)\n",
    "- [Evaluate your Generative AI application locally with the Azure AI Evaluation SDK](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk#evaluating-direct-and-indirect-attack-jailbreak-vulnerability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc35897",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "from promptflow.tracing import start_trace\n",
    "\n",
    "if \"AZURE_OPENAI_API_KEY\" not in os.environ:\n",
    "    # load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "# start a trace session, and print a url for user to check trace\n",
    "start_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03acbd3",
   "metadata": {},
   "source": [
    "## Setup Credentials and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure credentials\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Create an Azure project configuration\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_AI_FOUNDRY_PROJECT\"),\n",
    "}\n",
    "\n",
    "# Create a model configuration\n",
    "model_config = {\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}\n",
    "\n",
    "# Create an Azure OpenAI model configuration\n",
    "configuration = AzureOpenAIModelConfiguration(\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad60c55",
   "metadata": {},
   "source": [
    "## Groundedness Evaluator and Groundedness Pro Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GroundednessProEvaluator, GroundednessEvaluator\n",
    "\n",
    "# Initialazing Groundedness and Groundedness Pro evaluators\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "groundedness_pro_eval = GroundednessProEvaluator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "\n",
    "query_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    context=\"The Alpine Explorer Tent is the second most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "# Running Groundedness Evaluator on a query and response pair\n",
    "groundedness_score = groundedness_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(groundedness_score)\n",
    "\n",
    "groundedness_pro_score = groundedness_pro_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(groundedness_pro_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1606cb",
   "metadata": {},
   "source": [
    "## Answer Length Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from answer_length.answer_length import AnswerLengthEvaluator\n",
    "answer_length_evaluator = AnswerLengthEvaluator()\n",
    "answer_length = answer_length_evaluator(answer=\"What is the speed of light?\")\n",
    "\n",
    "print(answer_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friendliness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from friendliness.friendliness import FriendlinessEvaluator\n",
    "\n",
    "friendliness_eval = FriendlinessEvaluator(configuration)\n",
    "\n",
    "friendliness_score = friendliness_eval(response=\"I will not apologize for my behavior!\")\n",
    "print(friendliness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2808a",
   "metadata": {},
   "source": [
    "## Evaluate with both built-in and custom evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"./data/data.csv\", # provide your data here\n",
    "    evaluators={\n",
    "        \"groundedness\": groundedness_eval,\n",
    "        \"answer_length\": answer_length_evaluator,\n",
    "        \"friendliness\": friendliness_eval\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"answer_length\": {\n",
    "            \"column_mapping\": {\n",
    "                \"answer\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"friendliness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # Optionally provide your Azure AI project information to track your evaluation results in your Azure AI project\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    # # Optionally provide an output path to dump a json of metric summary, row level data and metric and Azure AI project URL\n",
    "    output_path=\"./results.json\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
